<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Research Project | Sam Wu</title>
  <script src="https://cdn.tailwindcss.com"></script>
</head>
<body class="bg-gray-100 text-gray-800">
  <header class="bg-blue-900 text-white py-4">
  <div class="max-w-4xl mx-auto px-6 flex items-center justify-between">
    <a href="index.html" class="text-2xl font-bold hover:underline">Sam Wu</a>
    <button class="md:hidden" id="menu-button">
      <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"></path>
      </svg>
    </button>
  </div>
  <nav class="hidden md:flex max-w-4xl mx-auto px-6 flex-wrap justify-between gap-x-6 text-sm md:text-base mt-4 md:mt-2 md:flex-row flex-col text-center" id="menu">
  <a href="projects.html" class="hover:underline block py-1 font-semibold">Research Project</a>
  <a href="experience.html" class="hover:underline block py-1">Work Experience</a>
  <a href="volunteer.html" class="hover:underline block py-1">Volunteer Activity</a>
  <a href="honors.html" class="hover:underline block py-1">Honors and Awards</a>
</nav>
  <script>
    document.addEventListener('DOMContentLoaded', function () {
      const button = document.getElementById('menu-button');
      const menu = document.getElementById('menu');
      button.addEventListener('click', () => {
        menu.classList.toggle('hidden');
      });
    });
  </script>
</header>

  
  <main class="max-w-4xl mx-auto p-6 space-y-8">

    <!-- Project 1 -->
    <section>
      <h2 class="text-xl font-bold mb-1 flex items-center gap-3">
  Robot Arm Manipulation - Tower of Hanoi Project
  <a href="https://github.com/samwu0217/Robot-Arm-Manipulation---Tower-of-Hanoi" target="_blank" rel="noopener noreferrer" class="text-gray-500 hover:text-black transition-colors" aria-label="View on GitHub">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" class="w-6 h-6">
      <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/>
    </svg>
  </a>
  </h2>
      
      <p class="text-sm text-gray-600 mb-1 italic">Term Project – Robotics (PME 526000)</p>
      <p class="italic text-sm mb-2">Advisor: <a href="https://pme.site.nthu.edu.tw/p/406-1308-180016,r4027.php?Lang=zh-tw" target="_blank" class="text-blue-600 underline">Dr. Ting-Jen Yeh</a></p>
      <p class="mb-4">
        This project focused on developing a robotic arm system capable of completing the Tower of Hanoi task through simulation and real-world execution. The robot arm was modeled in URDF and simulated in RViz with MoveIt for path planning and obstacle avoidance (via RRT algorithm). A custom IK solver was developed to compute joint configurations for desired end-effector poses. The solution was validated both in simulation and on physical hardware, with real-time control achieved via serial communication with a microcontroller running embedded motor control logic.
      </p>
      <p class="mb-4">
        In addition, this project integrated a GPT-based natural language interface and a computer vision module utilizing OpenCV, enabling the robot to interpret verbal instructions and recognize objects based on shape and position. This multimodal approach lays the foundation for autonomous object manipulation through intuitive human-robot interaction.
      </p>
      <p class="text-sm text-gray-500 mb-4">
        <strong>Keywords:</strong> ROS, robot arm, GPT-4o, OpenCV, computer vision, inverse kinematics
      </p>
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
        <img src="images/robot_1.jpg" class="rounded-lg shadow w-full h-64 object-cover">
        <img src="images/robot_2.png" class="rounded-lg shadow w-full h-64 object-cover">
      </div>
    </section>

    <!-- Project 2 -->
    <section>
      <h2 class="text-xl font-bold mb-1 flex items-center gap-3">
  Lane Prediction Model Training
  <a href="https://github.com/samwu0217/Lane-Prediction-Model-Training" target="_blank" rel="noopener noreferrer" class="text-gray-500 hover:text-black transition-colors" aria-label="View on GitHub">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" class="w-6 h-6">
      <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/>
    </svg>
  </a>
  </h2>
    
      <p class="text-sm text-gray-600 mb-1 italic">Term Project – Introduction to Artificial Intelligence (PME 543900)</p>
      <p class="italic text-sm mb-2">Advisor: <a href="https://icms.site.nthu.edu.tw/p/404-1529-203458.php" target="_blank" class="text-blue-600 underline">Dr. Jinn-Liang Liu</a></p>
      <p class="mb-4">
        This project aims to reduce data and computational requirements while achieving performance comparable to that of a pre-trained supercombo model for autonomous driving perception. A custom neural network architecture, ModelB6, is developed with convolutional and recurrent layers to extract spatial and temporal features from sequential driving video frames. To support multi-task learning—including path prediction, lane detection, and lead vehicle tracking—a composite loss function is formulated using multiple weighted mean squared errors (MSEs). Training is optimized with a cosine decay learning rate schedule with warm-up to enhance convergence stability.
      </p>
      <p class="mb-4">
        The model is trained on a dataset of YUV-processed video sequences and evaluated through both quantitative loss metrics and visual overlay comparisons. A simulation framework is used to verify the predictions by rendering output trajectories and lane markers on input frames. Results demonstrate stable convergence and satisfactory accuracy across several outputs, though some targets show signs of underfitting or over-regularization. The proposed framework offers a lightweight yet effective solution for vision-based behavior prediction in autonomous driving systems.
      </p>
      <p class="text-sm text-gray-500 mb-4">
        <strong>Keywords:</strong> Machine learning, lane prediction, CNNs, cosine learning rate decay
      </p>
      <div class="flex justify-center">
        <img src="images/lane_1.png" class="w-[95%] md:w-[80%] lg:w-[70%] h-auto rounded-lg mb-4">
      </div>
    </section>

    <!-- Project 3 -->
    <section>
      <h2 class="text-xl font-bold mb-1">Development of Interface Circuit for Piezoelectric Mode-Matched MEMS Gyroscope</h2>
      <p class="text-sm text-gray-600 mb-1 italic">Capstone Project</p>
      <p class="italic text-sm mb-2">Advisor: <a href="https://pme.site.nthu.edu.tw/p/406-1308-180024,r4027.php?Lang=zh-tw" target="_blank" class="text-blue-600 underline">Dr. Sheng-Shian Li</a></p>
      <p class="mb-4">
        This project presents the development of a compact, modular interface circuit for a piezoelectric mode-matched MEMS gyroscope, aiming to replace bulky commercial instruments with a portable and integrated solution. The system features a self-oscillating drive circuit based on the Barkhausen criterion to excite the primary resonant mode at 10 MHz using ambient noise. The sensing circuit implements a mixer-based quadrature demodulation scheme, followed by an active low-pass filter to extract the angular velocity signal while suppressing quadrature and high-frequency noise. Supporting circuits include a precision voltage regulator providing bipolar supplies and a custom-built motorized rate table that emulates single-axis rotation for on-bench testing.
      </p>
      <p class="mb-4">
        The modular subsystems were verified individually and then connected for full system integration. Although signal attenuation due to impedance mismatch limited visibility on oscilloscopes, angular velocity signals were successfully observed using a spectrum analyzer. The system demonstrates the feasibility of low-cost, PCB-based signal processing for MEMS gyroscopes, offering potential for portable and scalable inertial sensing solutions.
      </p>
      <p class="text-sm text-gray-500 mb-4">
        <strong>Keywords:</strong> MEMS gyroscope, interface circuit, quadrature demodulation, angular velocity sensing, PCB system integration
      </p>
      <div class="flex justify-center">
        <img src="images/project_3.jpg" class="w-[105%] md:w-[80%] lg:w-[70%] h-auto rounded-lg mb-4">
      </div>

    </section>
    
    <!-- Project 4 -->
    <section>
      <h2 class="text-xl font-bold mb-1">Design a Low-Power Fully Differential CMOS Amplifier</h2>
      <p class="text-sm text-gray-600 mb-1 italic">Term Project - Electronics II (PME 320200)</p>
      <p class="italic text-sm mb-2">Advisor: <a href="https://pme.site.nthu.edu.tw/p/406-1308-180024,r4027.php?Lang=zh-tw" target="_blank" class="text-blue-600 underline">Dr. Sheng-Shian Li</a></p>
      <p class="mb-4">
        This project presents the design of a low-power, fully differential CMOS amplifier implemented in a 0.18 μm process, targeting high-precision analog applications. The amplifier architecture features an operational amplifier core with pseudo-resistors for input biasing and capacitors to define voltage gain. Transistors operating in the deep triode region are used to implement pseudo-resistors, enabling stable bias voltage control with minimal area and power consumption. Through iterative optimization of transistor sizing and compensation parameters, the design achieves a gain of 40.07 dB, a 3 dB bandwidth of 25.44 kHz, and a low total harmonic distortion (THD) of 0.4987%. While most design targets are met or exceeded, the power supply rejection ratio (PSRR) falls short at 50.4 dB due to the use of a resistor-based bias circuit. The proposed amplifier demonstrates excellent linearity, low input-referred noise (15.97 nV/√Hz), and strong common-mode rejection, making it suitable for low-noise analog front-end circuits. Further improvements are recommended through active biasing and enhanced CMFB design to improve PSRR and overall power efficiency.
      </p>
      <p class="text-sm text-gray-500 mb-4">
        <strong>Keywords:</strong> CMOS analog design, fully differential amplifier, THD reduction, PSRR, CMRR
      </p>
      <div class="flex justify-center">
        <img src="images/project_4.png" class="w-[90%] md:w-[80%] lg:w-[70%] h-auto rounded-lg mb-4">
      </div>

    </section>
    
  </main>

  <footer class="text-center text-sm text-gray-600 py-4 bg-gray-200 mt-12">
    &copy; 2025 Sam Wu. All rights reserved.
  </footer>
</body>
</html>
